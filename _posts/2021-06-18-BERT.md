---
title: "BERT Annotated Paper"
date: 2021-06-18T00:34:30-04:00
classes: wide
categories:
  - Annotated Paper
tags:
  - NLP
---


## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ##

![png](/images/BERT.png)


The revolutionary paper by Google that increased the State-of-the-art performance for various NLP tasks and
set the stepping stone for many other revolutionary architectures.

This paper leads the way and sets a direction for the entire domain. It shows clear benefits of using 
pre-trained models(trained on huge datasets) and transfer learning independent of the downstream tasks.


Please feel free to read along with the paper with my notes and highlights.

| Color | Meaning |
| :---: | :--- | 
| Green | Topics about the current paper |
| Yellow | Topics about other relevant references |
| Blue | Implementation details/ maths/experiments |
| Red | Text including my thoughts, questions, and understandings | 

I have added the architectural details, my insights on the transformer architecture 
and some idea about positional embeddings in the end.

<embed src="/assets/pdfs/BERT.pdf" width="1000px" height="2100px" />



You can also look into the introduction to BERT implementation with Tensorflow as:
- [Blog](https://au1206.github.io/tutorials/Fine_Tune_BERT_for_Text_Classification_with_TensorFlow/)
- [Kaggle-Notebook](https://www.kaggle.com/au1206/fine-tuning-bert-text-classification)

Please feel free to fork or follow the [GitHub](https://github.com/au1206/paper_annotations) Repo for all the Annotated Papers. 

**PS:** For now the PDF Above does not render properly on mobile device, so please download the pdf from the above button or get it from my [Github](https://github.com/au1206/paper_annotations)

<br>
CITATION
```
@inproceedings{47751,
              title	= {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
              author	= {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina N. Toutanova},
              year	= {2018},
              URL	= {https://arxiv.org/abs/1810.04805}
}

```