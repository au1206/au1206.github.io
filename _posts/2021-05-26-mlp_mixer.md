---
title: "ML-Mixer Annotated Paper"
date: 2021-05-26T15:34:30-04:00
categories:
  - Annotated Paper
tags:
  - CV
---

## ML-MIXER: An all MLP Architecture for Vision ##

A very recent paper which challenges the need for complicated transformer models for huge datasets and questions the inductive biases
in place for present image recognition tasks. This paper argues that given a huge dataset (size 100M+) the performance of traditional CNN based architectures or the new transformer based architectures are only marginally better than a classic MLP based architecture, thus questioning the inductive biases of both CNNs and Transformers for images.


Please feel free to read along the paper with my notes and highlights.

| Color | Meaning |
| :---: | :--- | 
| Green | Topics about the current paper |
| Yellow | Topics about other relevant references |
| Blue | Implementation details/ maths |
| Red | Text including my thoughts, questions, and understandings | 

<embed src="/assets/pdfs/mlp_mixer.pdf" width="1000px" height="2100px" />

<br>
CITATION
```
@misc{tolstikhin2021mlpmixer,
      title={MLP-Mixer: An all-MLP Architecture for Vision}, 
      author={Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Andreas Steiner and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey Dosovitskiy},
      year={2021},
      eprint={2105.01601},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```